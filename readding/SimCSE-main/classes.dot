digraph "classes" {
charset="utf-8"
rankdir=BT
"0" [label="{BertForCL|bert\llm_head\lmodel_args\l|forward(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, sent_emb, mlm_input_ids, mlm_labels)\l}", shape="record"];
"1" [label="{CLTrainer|control\ldeepspeed\llr_scheduler : NoneType\lmodel\lmodel_wrapped\loptimizer : NoneType\lstate\l|evaluate(eval_dataset: Optional[Dataset], ignore_keys: Optional[List[str]], metric_key_prefix: str, eval_senteval_transfer: bool): Dict[str, float]\ltrain(model_path: Optional[str], trial: Union['optuna.Trial', Dict[str, Any]])\l}", shape="record"];
"2" [label="{MLPLayer|activation\ldense\l|forward(features)\l}", shape="record"];
"3" [label="{Pooler|pooler_type\l|forward(attention_mask, outputs)\l}", shape="record"];
"4" [label="{RobertaForCL|lm_head\lmodel_args\lroberta\l|forward(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, sent_emb, mlm_input_ids, mlm_labels)\l}", shape="record"];
"5" [label="{SimCSE|device : Optional[str]\lindex : dict, NoneType\lis_faiss_index : bool\lmodel\lnum_cells : int\lnum_cells_in_search : int\lpooler : str, NoneType\ltokenizer\l|build_index(sentences_or_file_path: Union[str, List[str]], use_faiss: bool, faiss_fast: bool, device: str, batch_size: int)\lencode(sentence: Union[str, List[str]], device: str, return_numpy: bool, normalize_to_unit: bool, keepdim: bool, batch_size: int, max_length: int): Union[ndarray, Tensor]\lsearch(queries: Union[str, List[str]], device: str, threshold: float, top_k: int): Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]\lsimilarity(queries: Union[str, List[str]], keys: Union[str, List[str], ndarray], device: str): Union[float, ndarray]\l}", shape="record"];
"6" [label="{Similarity|cos\ltemp\l|forward(x, y)\l}", shape="record"];
"7" [label="{transformers.BertPreTrainedModel|base_model_prefix : str\lconfig_class\lload_tf_weights\l|}", shape="record"];
"8" [label="{transformers.PreTrainedModel|base_model\lbase_model_prefix : str\lconfig : PretrainedConfig\lconfig_class : NoneType\ldummy_inputs\lis_parallelizable : bool\lname_or_path\lvocab_size : Optional[int]\l|from_pretrained(cls: Optional[Union[str, os.PathLike]], pretrained_model_name_or_path)\lget_input_embeddings(): \lget_output_embeddings(): \linit_weights()\lprune_heads(heads_to_prune: Dict[int, List[int]])\lresize_token_embeddings(new_num_tokens: Optional[int]): \lsave_pretrained(save_directory: Union[str, os.PathLike])\lset_input_embeddings(value)\ltie_weights()\l}", shape="record"];
"9" [label="{transformers.Trainer|args : Optional[TrainingArguments]\lcallback_handler\lcompute_metrics : Optional[Callable[[EvalPrediction], Dict]]\lcompute_objective : NoneType\lcontrol\ldata_collator : NoneType\ldeepspeed : NoneType\leval_dataset : Optional[Dataset]\lfp16_backend : str, NoneType\lhp_name : NoneType, Optional[Callable[['optuna.Trial'], str]]\lhp_search_backend : Optional[Union['str', HPSearchBackend]], NoneType\lhp_space : NoneType\lis_model_parallel : bool\llabel_names : list\llabel_smoother : NoneType\llr_scheduler : NoneType\lmodel : Optional[Union[PreTrainedModel, torch.nn.Module]]\lmodel_init : NoneType\lmodel_wrapped : Optional[Union[PreTrainedModel, torch.nn.Module]], NoneType\lobjective\loptimizer : NoneType\lscaler\lsharded_dpp : bool\lstate\ltokenizer : Optional['PreTrainedTokenizerBase']\ltrain_dataset : Optional[Dataset]\luse_amp : bool\luse_apex : bool\luse_tune_checkpoints : bool\l|add_callback(callback)\lcall_model_init(trial)\lcompute_loss(model, inputs)\lcreate_optimizer_and_scheduler(num_training_steps: int)\levaluate(eval_dataset: Optional[Dataset], ignore_keys: Optional[List[str]], metric_key_prefix: str): Dict[str, float]\lfloating_point_ops(inputs: Dict[str, Union[torch.Tensor, Any]])\lget_eval_dataloader(eval_dataset: Optional[Dataset]): DataLoader\lget_test_dataloader(test_dataset: Dataset): DataLoader\lget_train_dataloader(): DataLoader\lhyperparameter_search(hp_space: Optional[Callable[['optuna.Trial'], Dict[str, float]]], compute_objective: Optional[Callable[[Dict[str, float]], float]], n_trials: int, direction: str, backend: Optional[Union['str', HPSearchBackend]], hp_name: Optional[Callable[['optuna.Trial'], str]]): BestRun\lis_local_process_zero(): bool\lis_world_process_zero(): bool\llog(logs: Dict[str, float]): \lnum_examples(dataloader: DataLoader): int\lpop_callback(callback)\lpredict(test_dataset: Dataset, ignore_keys: Optional[List[str]], metric_key_prefix: str): PredictionOutput\lprediction_loop(dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool], ignore_keys: Optional[List[str]], metric_key_prefix: str): PredictionOutput\lprediction_step(model, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]): Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]\lremove_callback(callback)\lsave_model(output_dir: Optional[str])\lstore_flos()\ltrain(model_path: Optional[str], trial: Union['optuna.Trial', Dict[str, Any]])\ltraining_step(model, inputs: Dict[str, Union[torch.Tensor, Any]]): \l}", shape="record"];
"10" [label="{transformers.RobertaPreTrainedModel|base_model_prefix : str\lconfig_class\l|}", shape="record"];
"11" [label="{transformers.BertTokenizerFast|do_lower_case : bool\lmax_model_input_sizes : dict\lpretrained_init_configuration : dict\lpretrained_vocab_files_map : dict\lslow_tokenizer_class\lvocab_files_names : dict\l|build_inputs_with_special_tokens(token_ids_0, token_ids_1)\lcreate_token_type_ids_from_sequences(token_ids_0: List[int], token_ids_1: Optional[List[int]]): List[int]\lsave_vocabulary(save_directory: str, filename_prefix: Optional[str]): Tuple[str]\l}", shape="record"];
"12" [label="{transformers.PreTrainedTokenizerFast|backend_tokenizer\ldecoder\lis_fast\lslow_tokenizer_class : Optional[PreTrainedTokenizer]\lvocab\lvocab_size\l|convert_ids_to_tokens(ids: Union[int, List[int]], skip_special_tokens: bool): Union[str, List[str]]\lconvert_tokens_to_ids(tokens: Union[str, List[str]]): Union[int, List[int]]\lconvert_tokens_to_string(tokens: List[str]): str\lget_added_vocab(): Dict[str, int]\lget_vocab(): Dict[str, int]\lnum_special_tokens_to_add(pair: bool): int\lset_truncation_and_padding(padding_strategy: PaddingStrategy, truncation_strategy: TruncationStrategy, max_length: int, stride: int, pad_to_multiple_of: Optional[int])\ltokenize(text: str, pair: Optional[str], add_special_tokens: bool): List[str]\l}", shape="record"];
"13" [label="{transformers.PreTrainedTokenizerBase|deprecation_warnings : dict\linit_inputs : tuple\linit_kwargs : dict\lmax_len_sentences_pair\lmax_len_single_sentence\lmax_model_input_sizes : Dict[str, Optional[int]]\lmodel_input_names\lmodel_input_names : List[str]\lmodel_max_length : int\lname_or_path\lpadding_side\lpadding_side : str\lpretrained_init_configuration : Dict[str, Dict[str, Any]]\lpretrained_vocab_files_map : Dict[str, Dict[str, str]]\lslow_tokenizer_class : NoneType\lvocab_files_names : Dict[str, str]\l|as_target_tokenizer()\lbatch_decode(sequences: Union[List[int], List[List[int]], 'np.ndarray', 'torch.Tensor', 'tf.Tensor'], skip_special_tokens: bool, clean_up_tokenization_spaces: bool): List[str]\lbatch_encode_plus(batch_text_or_text_pairs: Union[List[TextInput], List[TextInputPair], List[PreTokenizedInput], List[PreTokenizedInputPair], List[EncodedInput], List[EncodedInputPair]], add_special_tokens: bool, padding: Union[bool, str, PaddingStrategy], truncation: Union[bool, str, TruncationStrategy], max_length: Optional[int], stride: int, is_split_into_words: bool, pad_to_multiple_of: Optional[int], return_tensors: Optional[Union[str, TensorType]], return_token_type_ids: Optional[bool], return_attention_mask: Optional[bool], return_overflowing_tokens: bool, return_special_tokens_mask: bool, return_offsets_mapping: bool, return_length: bool, verbose: bool): BatchEncoding\lbuild_inputs_with_special_tokens(token_ids_0: List[int], token_ids_1: Optional[List[int]]): List[int]\lclean_up_tokenization(out_string): str\lconvert_tokens_to_string(tokens: List[str]): str\lcreate_token_type_ids_from_sequences(token_ids_0: List[int], token_ids_1: Optional[List[int]]): List[int]\ldecode(token_ids: Union[int, List[int], 'np.ndarray', 'torch.Tensor', 'tf.Tensor'], skip_special_tokens: bool, clean_up_tokenization_spaces: bool): str\lencode(text: Union[TextInput, PreTokenizedInput, EncodedInput], text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]], add_special_tokens: bool, padding: Union[bool, str, PaddingStrategy], truncation: Union[bool, str, TruncationStrategy], max_length: Optional[int], stride: int, return_tensors: Optional[Union[str, TensorType]]): List[int]\lencode_plus(text: Union[TextInput, PreTokenizedInput, EncodedInput], text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]], add_special_tokens: bool, padding: Union[bool, str, PaddingStrategy], truncation: Union[bool, str, TruncationStrategy], max_length: Optional[int], stride: int, is_split_into_words: bool, pad_to_multiple_of: Optional[int], return_tensors: Optional[Union[str, TensorType]], return_token_type_ids: Optional[bool], return_attention_mask: Optional[bool], return_overflowing_tokens: bool, return_special_tokens_mask: bool, return_offsets_mapping: bool, return_length: bool, verbose: bool): BatchEncoding\lfrom_pretrained(cls: Union[str, os.PathLike], pretrained_model_name_or_path)\lget_special_tokens_mask(token_ids_0: List[int], token_ids_1: Optional[List[int]], already_has_special_tokens: bool): List[int]\lget_vocab(): Dict[str, int]\lnum_special_tokens_to_add(pair: bool): int\lpad(encoded_inputs: Union[BatchEncoding, List[BatchEncoding], Dict[str, EncodedInput], Dict[str, List[EncodedInput]], List[Dict[str, EncodedInput]]], padding: Union[bool, str, PaddingStrategy], max_length: Optional[int], pad_to_multiple_of: Optional[int], return_attention_mask: Optional[bool], return_tensors: Optional[Union[str, TensorType]], verbose: bool): BatchEncoding\lprepare_for_model(ids: List[int], pair_ids: Optional[List[int]], add_special_tokens: bool, padding: Union[bool, str, PaddingStrategy], truncation: Union[bool, str, TruncationStrategy], max_length: Optional[int], stride: int, pad_to_multiple_of: Optional[int], return_tensors: Optional[Union[str, TensorType]], return_token_type_ids: Optional[bool], return_attention_mask: Optional[bool], return_overflowing_tokens: bool, return_special_tokens_mask: bool, return_offsets_mapping: bool, return_length: bool, verbose: bool, prepend_batch_axis: bool): BatchEncoding\lprepare_seq2seq_batch(src_texts: List[str], tgt_texts: Optional[List[str]], max_length: Optional[int], max_target_length: Optional[int], padding: str, return_tensors: str, truncation: bool): BatchEncoding\lsave_pretrained(save_directory: Union[str, os.PathLike], legacy_format: bool, filename_prefix: Optional[str]): Tuple[str]\lsave_vocabulary(save_directory: str, filename_prefix: Optional[str]): Tuple[str]\ltokenize(text: str, pair: Optional[str], add_special_tokens: bool): List[str]\ltruncate_sequences(ids: List[int], pair_ids: Optional[List[int]], num_tokens_to_remove: int, truncation_strategy: Union[str, TruncationStrategy], stride: int): Tuple[List[int], List[int], List[int]]\l}", shape="record"];
"0" -> "7" [arrowhead="empty", arrowtail="none"];
"7" -> "8" [arrowhead="empty", arrowtail="none"];
"1" -> "9" [arrowhead="empty", arrowtail="none"];
"4" -> "10" [arrowhead="empty", arrowtail="none"];
"10" -> "8" [arrowhead="empty", arrowtail="none"];
"11" -> "12" [arrowhead="empty", arrowtail="none"];
"12" -> "13" [arrowhead="empty", arrowtail="none"];
"0" -> "9" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="model", style="solid"];
"4" -> "9" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="model", style="solid"];
"11" -> "5" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="tokenizer", style="solid"];
"3" -> "0" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="pool", style="solid"];
"3" -> "4" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="pool", style="solid"];
"2" -> "0" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="mlp", style="solid"];
"2" -> "4" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="mlp", style="solid"];
"6" -> "0" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="sim", style="solid"];
"6" -> "4" [arrowhead="diamond", arrowtail="none", fontcolor="green", label="sim", style="solid"];
}
