digraph "classes" {
charset="utf-8"
rankdir=BT
"0" [label="{BertForCL|bert\llm_head\lmodel_args\l|forward(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, sent_emb, mlm_input_ids, mlm_labels)\l}", shape="record"];
"1" [label="{CLTrainer|control\ldeepspeed\llr_scheduler : NoneType\lmodel\lmodel_wrapped\loptimizer : NoneType\lstate\l|evaluate(eval_dataset: Optional[Dataset], ignore_keys: Optional[List[str]], metric_key_prefix: str, eval_senteval_transfer: bool): Dict[str, float]\ltrain(model_path: Optional[str], trial: Union['optuna.Trial', Dict[str, Any]])\l}", shape="record"];
"2" [label="{MLPLayer|activation\ldense\l|forward(features)\l}", shape="record"];
"3" [label="{Pooler|pooler_type\l|forward(attention_mask, outputs)\l}", shape="record"];
"4" [label="{RobertaForCL|lm_head\lmodel_args\lroberta\l|forward(input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, sent_emb, mlm_input_ids, mlm_labels)\l}", shape="record"];
"5" [label="{SimCSE|device : Optional[str]\lindex : dict, NoneType\lis_faiss_index : bool\lmodel\lnum_cells : int\lnum_cells_in_search : int\lpooler : str, NoneType\ltokenizer\l|build_index(sentences_or_file_path: Union[str, List[str]], use_faiss: bool, faiss_fast: bool, device: str, batch_size: int)\lencode(sentence: Union[str, List[str]], device: str, return_numpy: bool, normalize_to_unit: bool, keepdim: bool, batch_size: int, max_length: int): Union[ndarray, Tensor]\lsearch(queries: Union[str, List[str]], device: str, threshold: float, top_k: int): Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]\lsimilarity(queries: Union[str, List[str]], keys: Union[str, List[str], ndarray], device: str): Union[float, ndarray]\l}", shape="record"];
"6" [label="{Similarity|cos\ltemp\l|forward(x, y)\l}", shape="record"];
"7" [label="{transformers.BertPreTrainedModel|base_model_prefix : str\lconfig_class\lload_tf_weights\l|}", shape="record"];
"8" [label="{transformers.PreTrainedModel|base_model\lbase_model_prefix : str\lconfig : PretrainedConfig\lconfig_class : NoneType\ldummy_inputs\lis_parallelizable : bool\lname_or_path\lvocab_size : Optional[int]\l|from_pretrained(cls: Optional[Union[str, os.PathLike]], pretrained_model_name_or_path)\lget_input_embeddings(): \lget_output_embeddings(): \linit_weights()\lprune_heads(heads_to_prune: Dict[int, List[int]])\lresize_token_embeddings(new_num_tokens: Optional[int]): \lsave_pretrained(save_directory: Union[str, os.PathLike])\lset_input_embeddings(value)\ltie_weights()\l}", shape="record"];
"9" [label="{transformers.Trainer|args : Optional[TrainingArguments]\lcallback_handler\lcompute_metrics : Optional[Callable[[EvalPrediction], Dict]]\lcompute_objective : NoneType\lcontrol\ldata_collator : NoneType\ldeepspeed : NoneType\leval_dataset : Optional[Dataset]\lfp16_backend : str, NoneType\lhp_name : NoneType, Optional[Callable[['optuna.Trial'], str]]\lhp_search_backend : Optional[Union['str', HPSearchBackend]], NoneType\lhp_space : NoneType\lis_model_parallel : bool\llabel_names : list\llabel_smoother : NoneType\llr_scheduler : NoneType\lmodel : Optional[Union[PreTrainedModel, torch.nn.Module]]\lmodel_init : NoneType\lmodel_wrapped : Optional[Union[PreTrainedModel, torch.nn.Module]], NoneType\lobjective\loptimizer : NoneType\lscaler\lsharded_dpp : bool\lstate\ltokenizer : Optional['PreTrainedTokenizerBase']\ltrain_dataset : Optional[Dataset]\luse_amp : bool\luse_apex : bool\luse_tune_checkpoints : bool\l|add_callback(callback)\lcall_model_init(trial)\lcompute_loss(model, inputs)\lcreate_optimizer_and_scheduler(num_training_steps: int)\levaluate(eval_dataset: Optional[Dataset], ignore_keys: Optional[List[str]], metric_key_prefix: str): Dict[str, float]\lfloating_point_ops(inputs: Dict[str, Union[torch.Tensor, Any]])\lget_eval_dataloader(eval_dataset: Optional[Dataset]): DataLoader\lget_test_dataloader(test_dataset: Dataset): DataLoader\lget_train_dataloader(): DataLoader\lhyperparameter_search(hp_space: Optional[Callable[['optuna.Trial'], Dict[str, float]]], compute_objective: Optional[Callable[[Dict[str, float]], float]], n_trials: int, direction: str, backend: Optional[Union['str', HPSearchBackend]], hp_name: Optional[Callable[['optuna.Trial'], str]]): BestRun\lis_local_process_zero(): bool\lis_world_process_zero(): bool\llog(logs: Dict[str, float]): \lnum_examples(dataloader: DataLoader): int\lpop_callback(callback)\lpredict(test_dataset: Dataset, ignore_keys: Optional[List[str]], metric_key_prefix: str): PredictionOutput\lprediction_loop(dataloader: DataLoader, description: str, prediction_loss_only: Optional[bool], ignore_keys: Optional[List[str]], metric_key_prefix: str): PredictionOutput\lprediction_step(model, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]]): Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]\lremove_callback(callback)\lsave_model(output_dir: Optional[str])\lstore_flos()\ltrain(model_path: Optional[str], trial: Union['optuna.Trial', Dict[str, Any]])\ltraining_step(model, inputs: Dict[str, Union[torch.Tensor, Any]]): \l}", shape="record"];
"10" [label="{transformers.RobertaPreTrainedModel|base_model_prefix : str\lconfig_class\l|}", shape="record"];
"0" -> "7" [arrowhead="empty", arrowtail="none"];
"7" -> "8" [arrowhead="empty", arrowtail="none"];
"1" -> "9" [arrowhead="empty", arrowtail="none"];
"4" -> "10" [arrowhead="empty", arrowtail="none"];
"10" -> "8" [arrowhead="empty", arrowtail="none"];
}
