----

1. 样本方差的无偏估计为什么是n-1, 因为样本均值是用n来做的，那么当前面的n-1确定了，那么最后一个数值也确定了，`torch.var(unbaised=True)`
1. 一阶距和二阶距，python 中计算距的`from scipy.stats import moment`
1. 特征值和特征向量: [特征值代表运动的速度，特征向量代表运动的方向](https://www.zhihu.com/question/21874816) python计算 `np.linalg.eig(x)`
1. 计算hessian 矩阵，以函数`f(x)=b^Tx+0.5x^TAx` torch 计算见[代码部分](https://my.oschina.net/u/4344316/blog/3360747)
1. 方向导数是各个方向上的导数
1. 偏导数连续才有梯度存在
1. 梯度的方向是方向导数中取到最大值的方向，梯度的值是方向导数的最大值

------

# 概率论与数理统计

----

1. 概率论的主要内容是从分布求概率，数理统计的主要内容从样本推断总体
1. 数理统计的三块内容：参数估计，假设检验，回归分析
1. 参数估计的三种方法：距估计，极大似然估计，贝吔si估计
1.

----

```
# 计算hessian矩阵
import torch


# 定义函数
x = torch.tensor([0., 0, 0], requires_grad=True)
b = torch.tensor([1., 3, 5])
A = torch.tensor([[-5, -3, -0.5], [-3, -2, 0], [-0.5, 0, -0.5]])
y = b@x + 0.5*x@A@x

# 计算一阶导数,因为我们需要继续计算二阶导数,所以创建并保留计算图
grad = torch.autograd.grad(y, x, retain_graph=True, create_graph=True)
# 定义Print数组,为输出和进一步利用Hessian矩阵作准备
Print = torch.tensor([])
for anygrad in grad[0]:  # torch.autograd.grad返回的是元组
    Print = torch.cat((Print, torch.autograd.grad(anygrad, x, retain_graph=True)[0]))
print(Print.view(x.size()[0], -1))
```