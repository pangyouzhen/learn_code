# 基础模型

## 基础知识

### Normalization

-------

1. Normalization 主要是将数据进行白化（独立同分布）操作。通用框架是先归一化，然后再进行平移和伸缩变换
1. BatchNorm ：纵向规范化. 适用于每个 mini-batch 比较大，数据分布比较接近。训练前要进行 shuffle。但不适用于动态的网络结构和RNN
1. LayerNorm : 横向规范化. 

-------

### 激活函数

sigmoid 和 softmax的区别

1. sigmoid 一般用于多标签分类
1. softmax 一般用于多分类 因为softmax会对整体的概率进行归一化

### 优化器

#### SGD

#### Adam

-----

1. Adam 全名：adaptive moment estimation。适应性矩估计。
1. 随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率。
1.

-----

## CNN

## RNN

## LSTM

lstm 与 GRU的区别

1. 它将忘记门和输入门合成了一个单一的 更新门。同样还混合了细胞状态和隐藏状态，和其他一些改动。
1. 最终的模型比标准的 LSTM模型要简单。效果和LSTM差不多，但是参数少了1/3，不容易过拟合。

## ELMO

## transformer

----

1. attention 计算公式
1. multiattention 是多个attention的组合之后并进行concat
1. self-attention: attention 中的Q,K,V 都相等, 目的是学习句子内部的词依赖关系，捕获句子的内部结构

-----

![attention](../img/attention.png)

![multiattention](../img/multi_attention.png)

## Word2Vec